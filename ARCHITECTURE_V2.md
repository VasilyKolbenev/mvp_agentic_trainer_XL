# üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ ML Data Pipeline v2.0

## –û–±–∑–æ—Ä

–ü—Ä–æ–µ–∫—Ç —Ä–µ–æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω —Å–æ–≥–ª–∞—Å–Ω–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ ML Data Pipeline —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∏ PydanticAI –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ AI-–∞–≥–µ–Ω—Ç–æ–≤.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         ML Data Pipeline System                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ECK_Logs  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    API     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   ETL   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇLabeler_Agent ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂
‚îÇ(ClickHouse)‚îÇ    ‚îÇ (FastAPI)  ‚îÇ    ‚îÇ(–û–±—Ä–∞–±–æ—Ç)‚îÇ    ‚îÇ(LLM-–∞–≥–µ–Ω—Ç)   ‚îÇ    
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    
                                                           ‚îÇ
                                                           ‚ñº
                                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                                    ‚îÇReviewDataset ‚îÇ
                                                    ‚îÇ   (HITL)     ‚îÇ
                                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                         ‚îÇ                     ‚îÇ
                                         ‚ñº                     ‚ñº
                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                              ‚îÇ Augmenter_Agent    ‚îÇ    ‚îÇ          ‚îÇ
                              ‚îÇ (–°–∏–Ω—Ç–µ—Ç–∏–∫–∞)        ‚îÇ    ‚îÇ          ‚îÇ
                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ          ‚îÇ
                                         ‚îÇ              ‚îÇ          ‚îÇ
                                         ‚ñº              ‚ñº          ‚ñº
                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                    ‚îÇ      DataWriter              ‚îÇ
                                    ‚îÇ (train/eval —Å–ø–ª–∏—Ç)           ‚îÇ
                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                               ‚îÇ
                                               ‚ñº
                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                    ‚îÇ      DataStorage             ‚îÇ
                                    ‚îÇ  (–í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ)           ‚îÇ
                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã

### 1. ECK_Logs (–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö)

**–†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ:** ClickHouse Database

**–û–ø–∏—Å–∞–Ω–∏–µ:** –•—Ä–∞–Ω–∏–ª–∏—â–µ –ª–æ–≥–æ–≤ ESK (–ï–¥–∏–Ω—ã–π –°–µ—Ä–≤–∏—Å–Ω—ã–π –ö–æ–Ω—Ç–∞–∫—Ç-—Ü–µ–Ω—Ç—Ä) –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ –ø–æ—Å—Ç—É–ø–∞—é—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.

**–§—É–Ω–∫—Ü–∏–∏:**
- –•—Ä–∞–Ω–µ–Ω–∏–µ —Å—ã—Ä—ã—Ö –ª–æ–≥–æ–≤ –æ–±—Ä–∞—â–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
- SQL –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
- –≠–∫—Å–ø–æ—Ä—Ç –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö (CSV, JSON, Parquet)
- –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ

**–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ:**
```python
# –ü—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞ –∫ ClickHouse
import clickhouse_connect

client = clickhouse_connect.get_client(
    host='your-clickhouse-host',
    port=8123,
    username='default',
    password='password'
)

# –í—ã–≥—Ä—É–∑–∫–∞ –ª–æ–≥–æ–≤
query = """
    SELECT 
        text,
        domain,
        timestamp,
        user_id
    FROM esk_logs
    WHERE date >= today() - 7
"""

df = client.query_df(query)
# –î–∞–ª–µ–µ ‚Üí API ‚Üí ETL ‚Üí Pipeline
```

---

### 2. API (–í—Ö–æ–¥–Ω–∞—è —Ç–æ—á–∫–∞ Pipeline)

**–†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ:** `src/api.py`

**–û–ø–∏—Å–∞–Ω–∏–µ:** FastAPI REST API —Å–µ—Ä–≤–∏—Å –¥–ª—è –ø—Ä–∏–µ–º–∞ –ª–æ–≥–æ–≤ –∏–∑ ClickHouse –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è pipeline.

**–§—É–Ω–∫—Ü–∏–∏:**
- –ü—Ä–∏–µ–º —Ñ–∞–π–ª–æ–≤ —á–µ—Ä–µ–∑ HTTP (XLSX/CSV/JSON/JSONL/Parquet)
- –ü—Ä–∏–µ–º –¥–∞–Ω–Ω—ã—Ö –Ω–∞–ø—Ä—è–º—É—é –∏–∑ ClickHouse —á–µ—Ä–µ–∑ SQL connector
- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ API
- –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–µ—Ä—Å–∏—è–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
- –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- Swagger –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (`/docs`)

**–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏:**
- `FastAPI` –¥–ª—è REST API
- `uvicorn` –¥–ª—è ASGI —Å–µ—Ä–≤–µ—Ä–∞  
- `clickhouse-connect` –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ ClickHouse (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
- Async/await –¥–ª—è –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
- OpenAPI/Swagger –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∏–∑ –∫–æ—Ä–æ–±–∫–∏

---

### 2. ETL (Extract, Transform, Load)

**–†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ:** `src/pipeline/etl.py`

**–û–ø–∏—Å–∞–Ω–∏–µ:** –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –≤—Ö–æ–¥—è—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

**–§—É–Ω–∫—Ü–∏–∏:**
- –ß—Ç–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤: XLSX, CSV, JSON, JSONL, Parquet
- –ê–≤—Ç–æ–¥–µ—Ç–µ–∫—Ü–∏—è –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π
- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
- –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
- –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö

**–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:**
```python
class ETLConfig:
    max_rows: Optional[int] = None
    deduplicate: bool = True
    min_text_length: int = 3
    max_text_length: int = 1000
    remove_empty: bool = True
    normalize_whitespace: bool = True
```

**–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**
```python
from src.pipeline.etl import ETLProcessor, ETLConfig

config = ETLConfig(max_rows=10000, deduplicate=True)
processor = ETLProcessor(config)

df = processor.process_file(Path("data/logs.xlsx"))
# –†–µ–∑—É–ª—å—Ç–∞—Ç: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ [text, ts, user_id, source, metadata]
```

---

### 3. Labeler_Agent (LLM-–∞–≥–µ–Ω—Ç —Ä–∞–∑–º–µ—Ç–∫–∏)

**–†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ:** `src/pipeline/labeler_agent.py`

**–û–ø–∏—Å–∞–Ω–∏–µ:** AI-–∞–≥–µ–Ω—Ç –Ω–∞ –±–∞–∑–µ PydanticAI –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤ –¥–æ–º–µ–Ω–∞–º–∏.

**–§—É–Ω–∫—Ü–∏–∏:**
- Batch –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å rate limiting
- –¢–∏–ø–æ–±–µ–∑–æ–ø–∞—Å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —á–µ—Ä–µ–∑ Pydantic
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ few-shot –ø—Ä–∏–º–µ—Ä—ã
- –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ feedback
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö LLM –º–æ–¥–µ–ª–µ–π

**–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏:**
- **PydanticAI** - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è AI-–∞–≥–µ–Ω—Ç–æ–≤
- **Pydantic** v2.8+ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
- OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∞–≥–µ–Ω—Ç–∞:**
```python
class LabelerAgent:
    def __init__(self, config: LabelerConfig):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è PydanticAI –∞–≥–µ–Ω—Ç–∞
        self.agent = Agent(
            model=OpenAIModel(...),
            result_type=ClassificationResult,
            system_prompt=self._build_system_prompt()
        )
    
    async def classify_one(self, text: str) -> ClassificationResult:
        # –¢–∏–ø–æ–±–µ–∑–æ–ø–∞—Å–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        result = await self.agent.run(text)
        return result.data  # –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ ClassificationResult
```

**–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π:**
```python
# Ollama
config = LabelerConfig(
    api_base="http://localhost:11434/v1",
    model="llama3.1:8b",
    api_key="dummy"
)

# vLLM
config = LabelerConfig(
    api_base="http://localhost:8000/v1",
    model="microsoft/DialoGPT-large"
)

# LM Studio
config = LabelerConfig(
    api_base="http://localhost:1234/v1",
    model="local-model"
)
```

**–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:**
```python
class LabelerConfig:
    model: str = "gpt-4o-mini"
    api_key: str
    api_base: Optional[str] = None
    
    batch_size: int = 20
    rate_limit: float = 0.4
    max_retries: int = 3
    
    low_conf_threshold: float = 0.5
    use_cache: bool = True
    use_dynamic_fewshot: bool = True
```

---

### 4. Augmenter_Agent (–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è)

**–†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ:** `src/pipeline/augmenter_agent.py`

**–û–ø–∏—Å–∞–Ω–∏–µ:** AI-–∞–≥–µ–Ω—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤.

**–§—É–Ω–∫—Ü–∏–∏:**
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–æ–∫
- –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–æ–º–µ–Ω–∞–º
- –ö–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
- –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

**–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:**
```python
class AugmenterConfig:
    model: str = "gpt-4o-mini"
    api_key: str
    api_base: Optional[str] = None
    
    variants_per_sample: int = 3
    include_hard_negatives: bool = False
    
    concurrency: int = 8
    rate_limit: float = 0.1
    max_samples_per_domain: int = 30
```

**–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**
```python
from src.pipeline.augmenter_agent import AugmenterAgent, AugmenterConfig

config = AugmenterConfig(
    model="gpt-4o-mini",
    api_key="sk-...",
    variants_per_sample=3,
    concurrency=8
)

agent = AugmenterAgent(config)

# –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –±–∞—Ç—á–∞
items = [{"text": "–ø–µ—Ä–µ–¥–∞—Ç—å –ø–æ–∫–∞–∑–∞–Ω–∏—è", "domain_id": "house"}, ...]
synthetic_samples = await agent.augment_batch(items)

# –†–µ–∑—É–ª—å—Ç–∞—Ç: [AugmentedSample(...), ...]
```

---

### 5. ReviewDataset (Human-in-the-Loop)

**–†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ:** `src/pipeline/review_dataset.py`

**–û–ø–∏—Å–∞–Ω–∏–µ:** –ö–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä—É—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏.

**–§—É–Ω–∫—Ü–∏–∏:**
- –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
- –û—á–µ—Ä–µ–¥—å –Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫—É —Å —Å—Ç–∞—Ç—É—Å–∞–º–∏
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å–∏—Å—Ç–µ–º–æ–π –æ–±—É—á–µ–Ω–∏—è
- –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–∑–º–µ—Ç–∫–∏
- –≠–∫—Å–ø–æ—Ä—Ç –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

**–°—Ç—Ä—É–∫—Ç—É—Ä–∞ ReviewItem:**
```python
class ReviewItem:
    id: str
    text: str
    predicted_domain: str
    confidence: float
    top_candidates: List[List[Any]]
    
    corrected_domain: Optional[str]
    reviewer_id: Optional[str]
    review_timestamp: Optional[datetime]
    
    status: ReviewStatus  # PENDING, IN_REVIEW, APPROVED, CORRECTED
    priority: ReviewPriority  # LOW, MEDIUM, HIGH, CRITICAL
```

**–†–∞–±–æ—á–∏–π –ø—Ä–æ—Ü–µ—Å—Å:**
```python
from src.pipeline.review_dataset import ReviewDataset, ReviewDatasetConfig

config = ReviewDatasetConfig(
    data_dir=Path("data"),
    low_confidence_threshold=0.5,
    high_priority_threshold=0.3
)

review = ReviewDataset(config)

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –æ—á–µ—Ä–µ–¥—å
review.add_items(low_confidence_items)

# –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
items = review.get_next(count=1, reviewer_id="user123")

# –û—Ç–ø—Ä–∞–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
review.submit_review(
    item_id=items[0].id,
    corrected_domain="house",
    reviewer_id="user123"
)

# –≠–∫—Å–ø–æ—Ä—Ç –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
reviewed_path = review.export_reviewed()
```

---

### 6. DataWriter (–ó–∞–ø–∏—Å—å –¥–∞—Ç–∞—Å–µ—Ç–æ–≤)

**–†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ:** `src/pipeline/data_writer.py`

**–û–ø–∏—Å–∞–Ω–∏–µ:** –ö–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è –∑–∞–ø–∏—Å–∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö train/eval –¥–∞—Ç–∞—Å–µ—Ç–æ–≤.

**–§—É–Ω–∫—Ü–∏–∏:**
- –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–ª–∏—Ç –ø–æ –¥–æ–º–µ–Ω–∞–º
- –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤
- –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
- –®–∞—Ä–¥–∏–Ω–≥ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏

**–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:**
```python
class DataWriterConfig:
    output_dir: Path
    
    eval_fraction: float = 0.1
    min_eval_samples: int = 50
    min_samples_per_domain: int = 5
    
    balance_domains: bool = True
    max_samples_per_domain: Optional[int] = None
    
    shard_size: Optional[int] = None
    include_metadata: bool = True
    validate_quality: bool = True
```

**–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**
```python
from src.pipeline.data_writer import DataWriter, DataWriterConfig

config = DataWriterConfig(
    output_dir=Path("data/artifacts"),
    eval_fraction=0.1,
    balance_domains=True
)

writer = DataWriter(config)

# –ó–∞–ø–∏—Å—å –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
train_path, eval_path, stats = writer.write_datasets(
    items=all_labeled_items,
    dataset_name="dataset"
)

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
print(f"Train: {stats.train_samples}, Eval: {stats.eval_samples}")
print(f"Domains: {stats.domain_distribution}")
```

---

### 7. DataStorage (–í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ)

**–†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ:** `src/pipeline/data_storage.py`

**–û–ø–∏—Å–∞–Ω–∏–µ:** –ö–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Ö—Ä–∞–Ω–µ–Ω–∏—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤.

**–§—É–Ω–∫—Ü–∏–∏:**
- –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ (v1.2.3)
- Git-like –æ–ø–µ—Ä–∞—Ü–∏–∏ (commit, tag, checkout)
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–π –∏ diff
- –ê–≤—Ç–æ–∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π
- –≠–∫—Å–ø–æ—Ä—Ç/–∏–º–ø–æ—Ä—Ç –≤–µ—Ä—Å–∏–π

**–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞:**
```
storage_dir/
‚îú‚îÄ‚îÄ versions/
‚îÇ   ‚îú‚îÄ‚îÄ v1.0.0/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.jsonl
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ eval.jsonl
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metadata.json
‚îÇ   ‚îú‚îÄ‚îÄ v1.1.0/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ v2.0.0/
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ current -> versions/v2.0.0  (symlink)
‚îî‚îÄ‚îÄ versions.json  (—Ä–µ–µ—Å—Ç—Ä)
```

**–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**
```python
from src.pipeline.data_storage import DataStorage, DataStorageConfig

config = DataStorageConfig(
    storage_dir=Path("data/storage"),
    max_versions=100,
    auto_archive_old=True
)

storage = DataStorage(config)

# –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏
version = storage.commit_version(
    train_path=Path("data/artifacts/dataset_train.jsonl"),
    eval_path=Path("data/artifacts/dataset_eval.jsonl"),
    version_tag="v1.2.0",  # –∏–ª–∏ auto-increment
    description="Added user feedback, balanced domains",
    status=VersionStatus.STABLE,
    created_by="bot_pipeline"
)

# –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ –≤–µ—Ä—Å–∏—é
storage.checkout("v1.2.0")

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–≥–∞
storage.tag_version("v1.2.0", "production")

# –°–ø–∏—Å–æ–∫ –≤–µ—Ä—Å–∏–π
versions = storage.list_versions(status=VersionStatus.STABLE)

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–π
diff = storage.compare_versions("v1.1.0", "v1.2.0")

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
stats = storage.get_stats()
print(f"Total versions: {stats['total_versions']}")
print(f"Storage size: {stats['total_size_mb']:.2f} MB")
```

---

## –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤

### –ü–æ–ª–Ω—ã–π pipeline (–ø—Ä–∏–º–µ—Ä)

```python
from pathlib import Path
from src.pipeline import (
    ETLProcessor, ETLConfig,
    LabelerAgent, LabelerConfig,
    AugmenterAgent, AugmenterConfig,
    ReviewDataset, ReviewDatasetConfig,
    DataWriter, DataWriterConfig,
    DataStorage, DataStorageConfig,
)

# 1. ETL - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
etl_config = ETLConfig(max_rows=10000, deduplicate=True)
etl = ETLProcessor(etl_config)
df = etl.process_file(Path("data/logs.xlsx"))

# 2. Labeler - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞
labeler_config = LabelerConfig(
    model="gpt-4o-mini",
    api_key="sk-...",
    batch_size=20,
    rate_limit=0.4
)
labeler = LabelerAgent(labeler_config)
results = await labeler.classify_dataframe(df)

# 3. Review - HITL –¥–ª—è –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
low_conf_items = labeler.get_low_confidence_items(results)

review_config = ReviewDatasetConfig(data_dir=Path("data"))
review = ReviewDataset(review_config)
review.add_items([r.dict() for r in low_conf_items])

# ... –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ HITL API endpoints –∏–ª–∏ –≤–Ω–µ—à–Ω–∏–π UI ...

reviewed_items = review.export_reviewed()

# 4. Augmenter - —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è
augmenter_config = AugmenterConfig(
    model="gpt-4o-mini",
    api_key="sk-...",
    variants_per_sample=3
)
augmenter = AugmenterAgent(augmenter_config)

high_conf_items = [r for r in results if r.confidence >= 0.7]
synthetic_items = await augmenter.augment_batch(
    [r.dict() for r in high_conf_items]
)

# 5. DataWriter - –∑–∞–ø–∏—Å—å train/eval
writer_config = DataWriterConfig(
    output_dir=Path("data/artifacts"),
    eval_fraction=0.1,
    balance_domains=True
)
writer = DataWriter(writer_config)

all_items = (
    [r.dict() for r in results] +
    [s.dict() for s in synthetic_items]
)

train_path, eval_path, stats = writer.write_datasets(
    all_items,
    dataset_name="dataset"
)

# 6. DataStorage - –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
storage_config = DataStorageConfig(
    storage_dir=Path("data/storage"),
    max_versions=100
)
storage = DataStorage(storage_config)

version = storage.commit_version(
    train_path=train_path,
    eval_path=eval_path,
    description="Balanced domains with synthetic augmentation",
    status=VersionStatus.STABLE
)

print(f"Created version: {version.version_tag}")
```

---

## –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### –ù–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

**–†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ:** `src/config_v2.py`

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç **Pydantic Settings** –¥–ª—è —Ç–∏–ø–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:

```python
from src.config_v2 import Settings

settings = Settings.load()

# App config
print(settings.app.mode)

# LLM config
print(settings.llm.model)
print(settings.llm.api_base)

# –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥–∏
print(settings.labeler.batch_size)
print(settings.augmenter.variants_per_sample)
```

### –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è

–°–æ–∑–¥–∞–π—Ç–µ `.env` —Ñ–∞–π–ª:

```env
# LLM (–≥–ª–∞–≤–Ω–∞—è –º–æ–¥–µ–ª—å)
LLM_API_KEY=sk-...
LLM_API_BASE=https://api.openai.com/v1
LLM_MODEL=gpt-4o-mini

# LLM Labeler (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—É—é –º–æ–¥–µ–ª—å)
LLM_LABELER_API_KEY=sk-...
LLM_LABELER_API_BASE=http://localhost:11434/v1
LLM_LABELER_MODEL=llama3.1:8b

# LLM Augmenter (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
LLM_AUGMENTER_API_KEY=sk-...
LLM_AUGMENTER_MODEL=gpt-4o-mini

# ETL
ETL_MAX_ROWS=10000
ETL_DEDUPLICATE=true

# Labeler
LABELER_BATCH_SIZE=20
LABELER_RATE_LIMIT=0.4
LABELER_LOW_CONF_THRESHOLD=0.5
LABELER_USE_CACHE=true

# Augmenter
AUGMENTER_VARIANTS_PER_SAMPLE=3
AUGMENTER_CONCURRENCY=8
AUGMENTER_RATE_LIMIT=0.1

# Review (HITL)
REVIEW_LOW_CONFIDENCE_THRESHOLD=0.5
REVIEW_MAX_QUEUE_SIZE=10000

# DataWriter
DATA_WRITER_EVAL_FRACTION=0.1
DATA_WRITER_BALANCE_DOMAINS=true

# DataStorage
DATA_STORAGE_MAX_VERSIONS=100
DATA_STORAGE_AUTO_ARCHIVE_OLD=true

# Cache
CACHE_TTL_HOURS=24
CACHE_ENABLED=true

# App
APP_MODE=production
APP_DATA_DIR=data
APP_LOG_LEVEL=INFO
```

---

## –õ–æ–∫–∞–ª—å–Ω—ã–µ LLM –º–æ–¥–µ–ª–∏

### –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ä–µ—à–µ–Ω–∏—è

#### 1. **Ollama**

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞
curl -fsSL https://ollama.ai/install.sh | sh

# –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞
ollama serve

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
ollama pull llama3.1:8b

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
LLM_API_BASE=http://localhost:11434/v1
LLM_MODEL=llama3.1:8b
LLM_API_KEY=dummy
```

#### 2. **vLLM**

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞
pip install vllm

# –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞
python -m vllm.entrypoints.openai.api_server \
  --model microsoft/DialoGPT-large \
  --port 8000

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
LLM_API_BASE=http://localhost:8000/v1
LLM_MODEL=microsoft/DialoGPT-large
```

#### 3. **LM Studio**

```bash
# 1. –ó–∞–ø—É—Å—Ç–∏—Ç—å LM Studio GUI
# 2. –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
# 3. –ó–∞–ø—É—Å—Ç–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
LLM_API_BASE=http://localhost:1234/v1
LLM_MODEL=local-model
LLM_API_KEY=dummy
```

#### 4. **Text Generation WebUI (Oobabooga)**

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞
git clone https://github.com/oobabooga/text-generation-webui
cd text-generation-webui
pip install -r requirements.txt

# –ó–∞–ø—É—Å–∫ —Å OpenAI API
python server.py --api --listen

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
LLM_API_BASE=http://localhost:5000/v1
LLM_MODEL=local-model
```

### –ê–¥–∞–ø—Ç–µ—Ä—ã –º–æ–¥–µ–ª–µ–π (–±—É–¥—É—â–∏–µ —É–ª—É—á—à–µ–Ω–∏—è)

–ü–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å –∞–¥–∞–ø—Ç–µ—Ä—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏:

- **Llama 3.1**: Chat template, stop tokens
- **Mistral**: Instruction template
- **Qwen**: Special tokens
- **GPT**: Native JSON mode

---

## –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

### 1. **–ú–æ–¥—É–ª—å–Ω–æ—Å—Ç—å**
- –ö–∞–∂–¥—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –Ω–µ–∑–∞–≤–∏—Å–∏–º –∏ —Ç–µ—Å—Ç–∏—Ä—É–µ–º
- –õ–µ–≥–∫–æ –∑–∞–º–µ–Ω–∏—Ç—å –∏–ª–∏ –æ–±–Ω–æ–≤–∏—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
- –ß–µ—Ç–∫–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏

### 2. **–¢–∏–ø–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å**
- Pydantic –¥–ª—è –≤—Å–µ—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
- PydanticAI –¥–ª—è —Ç–∏–ø–æ–±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö AI-–∞–≥–µ–Ω—Ç–æ–≤
- –ê–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –≤ IDE

### 3. **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å**
- –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
- –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º
- –®–∞—Ä–¥–∏–Ω–≥ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤

### 4. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**
- –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞–∂–¥–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
- –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π

### 5. **–ì–∏–±–∫–æ—Å—Ç—å**
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –∏ –æ–±–ª–∞—á–Ω—ã—Ö LLM
- –†–∞–∑–ª–∏—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á
- –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ –ø–∞–π–ø–ª–∞–π–Ω—ã

### 6. **Production-ready**
- –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏
- –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ fallback
- HITL –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞

---

## –ú–∏–≥—Ä–∞—Ü–∏—è —Å–æ —Å—Ç–∞—Ä–æ–π –≤–µ—Ä—Å–∏–∏

### –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å

–í—Å–µ –Ω–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç —Ñ—É–Ω–∫—Ü–∏–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å–æ —Å—Ç–∞—Ä—ã–º API:

```python
# –°—Ç–∞—Ä—ã–π API
from src.labeler import label_dataframe_batched

# –ù–æ–≤—ã–π API (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–¥ –∫–∞–ø–æ—Ç–æ–º)
from src.pipeline.labeler_agent import label_dataframe_batched

# –†–∞–±–æ—Ç–∞–µ—Ç –∏–¥–µ–Ω—Ç–∏—á–Ω–æ!
results = await label_dataframe_batched(df, llm_client, system_prompt, fewshot)
```

### –ü–æ—ç—Ç–∞–ø–Ω–∞—è –º–∏–≥—Ä–∞—Ü–∏—è

1. **–§–∞–∑–∞ 1**: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–æ–≤—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
   ```bash
   pip install -r requirements.txt
   ```

2. **–§–∞–∑–∞ 2**: –û–±–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
   ```python
   from src.config_v2 import Settings
   settings = Settings.load()
   ```

3. **–§–∞–∑–∞ 3**: –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –Ω–∞ –Ω–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
   - –ù–∞—á–∞—Ç—å —Å ETL
   - –ó–∞—Ç–µ–º Labeler/Augmenter
   - –í –∫–æ–Ω—Ü–µ DataWriter/Storage

4. **–§–∞–∑–∞ 4**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–ª–Ω—ã–π pipeline
   - –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
   - HITL –ø—Ä–æ—Ü–µ—Å—Å
   - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –º–µ—Ç—Ä–∏–∫

---

## –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

### –ó–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è

- [ ] **Multi-model routing** - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
- [ ] **Distributed processing** - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
- [ ] **MLOps integration** - –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å MLflow/Weights & Biases
- [ ] **A/B testing** - —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–π –º–æ–¥–µ–ª–µ–π
- [ ] **Real-time monitoring** - Prometheus/Grafana
- [ ] **API server** - REST API –¥–ª—è pipeline

---

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–ù–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ ML Data Pipeline v2.0 –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç:

‚úÖ **–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥** –∫ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—é ML —Å–∏—Å—Ç–µ–º
‚úÖ **Production-ready** –∫–æ–¥ —Å —Ç–∏–ø–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é
‚úÖ **–ì–∏–±–∫–æ—Å—Ç—å** –≤ –≤—ã–±–æ—Ä–µ –º–æ–¥–µ–ª–µ–π –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
‚úÖ **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å** –¥–ª—è —Ä–æ—Å—Ç–∞ –¥–∞–Ω–Ω—ã—Ö
‚úÖ **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥** –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –∫–∞–∫ –≤ development, —Ç–∞–∫ –∏ –≤ production –æ–∫—Ä—É–∂–µ–Ω–∏–∏.

